{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "# Create db connection.\n",
    "cnx = sqlite3.connect('data/database.sqlite')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seasons = ['2015/2016']\n",
    "leagues_id = [1729]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of df_matches of league:1729 and season:2015/2016 is: (380, 115)\n",
      "The shape of df_footballdata of league:1729 and season:15_16 is: (380, 65)\n",
      "df_complete_league_year shape: (380, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/core/frame.py:3140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "## Cleaning names of teams\n",
    "\n",
    "import stringdist\n",
    "\n",
    "# Normalize names \n",
    "def normalize_team_names(y):\n",
    "    teams = []\n",
    "    for t in list(y):\n",
    "        tt = t.lower().split()\n",
    "        rem = []\n",
    "        cont=0\n",
    "        #print(tt)\n",
    "        for el in tt:\n",
    "            cont +=1\n",
    "            if el in ['athletico','atletico','athlético','atlético','atl','ath','athletic']: \n",
    "                if cont < len(tt):\n",
    "                    tt[cont]='atl'+tt[cont]\n",
    "            if el in ['manchester','man','sporting','sp','deportivo','la','de','real','fc','cf','ud','las','cd','balompié','rc','de','athletico','atletico','athlético','atlético','atl','ath','athletic']:\n",
    "                rem.append(el)\n",
    "            \n",
    "                #print(tt)\n",
    "        if rem != []:\n",
    "            for r in rem:\n",
    "                tt.remove(r)\n",
    "        \n",
    "        if len(tt)>1:\n",
    "            if len(tt[0])>len(tt[1]):\n",
    "                t = tt[0]\n",
    "            else:\n",
    "                t = tt[1]\n",
    "        else:\n",
    "            t=tt[0]\n",
    "        #print(t)  \n",
    "        teams.append(t)\n",
    "    return teams\n",
    "\n",
    "#Change name x for the most similar name in the teams_normalized list.\n",
    "def func(x,teams_normalized):\n",
    "    dist =0\n",
    "    distmin=10\n",
    "    xt = x.lower().split()\n",
    "    rem = []\n",
    "    cont=0\n",
    "    for el in xt:\n",
    "        cont +=1\n",
    "        if el in ['athletico','atletico','athlético','atlético','atl','ath','athletic']:\n",
    "            if cont < len(xt):\n",
    "                xt[cont]='atl'+xt[cont]\n",
    "        if el in ['manchester','man','sporting','sp','deportivo','la','de','real','fc','cf','ud','las','cd','balompié','rc','de','athletico','atletico','athlético','atlético','atl','ath','athletic']:\n",
    "            rem.append(el)\n",
    "    if rem != []:\n",
    "        for r in rem:\n",
    "                xt.remove(r)\n",
    "                \n",
    "\n",
    "\n",
    "    if len(xt)>1:\n",
    "        if len(xt[0])>len(xt[1]):\n",
    "            x = xt[0]\n",
    "        else:\n",
    "            x = xt[1]\n",
    "    else:\n",
    "        x=xt[0]\n",
    "\n",
    "    \n",
    "    for t in list(teams_normalized):\n",
    "        dist = stringdist.levenshtein(x, t)\n",
    "        #print(t)\n",
    "        if dist < distmin:\n",
    "            #print('................')\n",
    "            #print(x)\n",
    "            #print(t)\n",
    "            #print(dist)\n",
    "            #print('................')\n",
    "            distmin = dist\n",
    "            team =t\n",
    "    return team\n",
    "\n",
    "\n",
    "\n",
    "def convert_to(df,type_to_convert, all_categorical = False,columns = 'None'):\n",
    " \n",
    "    if all_categorical == True:\n",
    "        categoric_types = df.select_dtypes(include=['object','category']).columns\n",
    "        if categoric_types.empty:\n",
    "            print('No object types in the dataframe to be converted')\n",
    "            return df\n",
    "        else:\n",
    "            if 'labels' in categoric_types:\n",
    "                categoric_types.remove('labels')\n",
    "            df[categoric_types] = df[categoric_types].astype('category')\n",
    "            columns = categoric_types\n",
    "        \n",
    "    else:\n",
    "        df[columns] = df[columns].astype('category')\n",
    "        \n",
    "        \n",
    "    if type_to_convert == 'labelling':\n",
    "        for column in columns:\n",
    "            df[column] = df[column].cat.codes\n",
    "    elif type_to_convert == 'onehot':\n",
    "        for column in columns:\n",
    "            print(columns)\n",
    "            df= pd.get_dummies(df, columns=[column])             \n",
    "      \n",
    "    return df\n",
    "\n",
    "def convert_to_standard_team_names(cnx, league_id, season):\n",
    "\n",
    "    #.......... df_matches ..........\n",
    "    \n",
    "    #Read matches from the season and league specified.\n",
    "    df_matches = pd.read_sql_query(\"SELECT * FROM Match WHERE league_id = (?) AND season = (?)\", cnx, params=(league_id,season,)) \n",
    "    print(f'The shape of df_matches of league:{league_id} and season:{season} is: {df_matches.shape}')\n",
    "\n",
    "    #Drop columns with not useful information (html text), we will get this information from another dataset from football-data.co.uk\n",
    "    df_matches = df_matches.drop(columns=['goal', 'shoton', 'shotoff','foulcommit', 'card', 'cross', 'corner', 'possession'])\n",
    "\n",
    "    #Convert types\n",
    "    df_matches['date'] = pd.to_datetime(df_matches['date'])\n",
    "    df_matches['date'] = pd.to_datetime(df_matches['date'],format='%Y%m%d')\n",
    "    df_matches['stage'] = df_matches['stage'].astype(int)\n",
    "    \n",
    "   #To merge data with the other dataset we need a common team id between both datasets. We are going to create a unique string identifier for team names named team_name_id.\n",
    "    \n",
    "    \n",
    "    #In the df_matches we will first include a column with the name of the team extracted from the Team table by means of the team_api_id.\n",
    "    df_Teams = pd.read_sql_query(\"SELECT * FROM Team \", cnx)\n",
    "    df_Teams.head()\n",
    "    \n",
    "\n",
    "    # Manually change some names due its difficulty to be treated for our string name procedure.\n",
    "    df_Teams.loc[df_Teams['team_long_name']=='Athletic Club de Bilbao','team_long_name'] = 'Athletic Bilbao'\n",
    "    df_Teams.loc[df_Teams['team_long_name']=='Real Sporting de Gijón','team_long_name'] = 'Sporting Gijon'\n",
    "    \n",
    "    df_Teams.loc[df_Teams['team_long_name']=='West Bromwich Albion','team_long_name'] = 'West Brom'\n",
    "    df_Teams.loc[df_Teams['team_long_name']=='West Ham United','team_long_name'] = 'West Ham'\n",
    "    df_Teams.loc[df_Teams['team_long_name']=='Queens Park Rangers','team_long_name'] = 'QPR'\n",
    "    df_Teams.loc[df_Teams['team_long_name']=='Stoke City','team_long_name'] = 'Stoke'\n",
    "    df_Teams.loc[df_Teams['team_long_name']=='Hull City','team_long_name'] = 'Hull'\n",
    "    df_Teams.loc[df_Teams['team_long_name']=='Bolton Wanderers','team_long_name'] = 'Bolton'\n",
    "    df_Teams.loc[df_Teams['team_long_name']=='Wolverhampton Wanderers', 'team_long_name'] = 'Wolves'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #First we normalize the names of the teams\n",
    "    teams_normalized = normalize_team_names(df_Teams['team_long_name'])\n",
    "    \n",
    "    #Now we normalize the names of df_Teams \n",
    "    df_Teams['team_name_id']= df_Teams['team_long_name'].apply(lambda x: func(x,teams_normalized))\n",
    "\n",
    "    \n",
    "    #Now we will have to include names of the teams into de df_matches dataframe, for this we have to merge df_matches with df_Teams on the \"team_api_id\".\n",
    "    #Since df_matches just have names of the 'home_team_api_id' and 'away_team_api_id', we will add a new column referring the corresponding teams as 'home_team_name_id' and 'away_team_name_id'.\n",
    "    df_matches['home_team_name_id']=df_matches.merge(df_Teams[['team_api_id','team_name_id']], left_on='home_team_api_id', right_on='team_api_id',how='left')['team_name_id']\n",
    "    df_matches['away_team_name_id']=df_matches.merge(df_Teams[['team_api_id','team_name_id']], left_on='away_team_api_id', right_on='team_api_id',how='left')['team_name_id']\n",
    "\n",
    "    #We finally have the df_matches with the Match Table information but with the home_tema_name_id and away_team_name_id which will be useful to create a common id between the two csv that we want to merge.\n",
    "    \n",
    "    #print(df_Teams)\n",
    "    \n",
    "\n",
    "    \n",
    "    #.......... footballdata ..........\n",
    "    \n",
    "    #Read matches from the season and league specified.\n",
    "    location_to_file = 'data/'\n",
    "    season_footdata = season.replace('/','_').replace('20','') \n",
    "    df_footballdata = pd.read_csv(location_to_file+'{0}_{1}.csv'.format(league_id,season_footdata))   \n",
    "    print(f'The shape of df_footballdata of league:{league_id} and season:{season_footdata} is: {df_footballdata.shape}')\n",
    "  \n",
    "    #Convert types    \n",
    "    df_footballdata['date'] = pd.to_datetime(df_footballdata['Date'])\n",
    "    df_footballdata['date'] = pd.to_datetime(df_footballdata['date'], format='%Y%m%d')\n",
    "\n",
    "    \n",
    "    #Creating same names for teams as the other dataset\n",
    "    df_footballdata['home_team_name_id'] = df_footballdata['HomeTeam'].apply(lambda x: func(x,teams_normalized))\n",
    "    df_footballdata['away_team_name_id'] = df_footballdata['AwayTeam'].apply(lambda x: func(x,teams_normalized))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Creating common match identifier for both datasets (uid)\n",
    "    df_footballdata['uid']= df_footballdata['home_team_name_id']+df_footballdata['away_team_name_id']\n",
    "    df_matches['uid']= df_matches['home_team_name_id']+df_matches['away_team_name_id']\n",
    "    \n",
    "    return df_matches, df_footballdata\n",
    "\n",
    "def merge_matchtable_footballdata(season,league_id):\n",
    "    \n",
    "    df_matches, df_footballdata = convert_to_standard_team_names(cnx, league_id, season)\n",
    "    \n",
    "#     print((df_matches[['home_team_name_id','away_team_name_id','uid']]))\n",
    "     \n",
    "#     print((df_footballdata[['home_team_name_id','away_team_name_id']]))\n",
    "\n",
    "#     print((df_matches['uid'].sort_values(ascending=True)))\n",
    "#     print((df_footballdata['uid'].sort_values(ascending=True)))\n",
    "    \n",
    "    if len(set(df_matches['uid']).symmetric_difference(set(df_footballdata['uid']))) > 0:\n",
    "        print('Names of uid of both datasets do not match.')\n",
    "        print('CAUTION!!!! THE FOLLOWING NAMES HAVE DIFFERENT SPELLING IN THE TWO DATASETS: ',\n",
    "              set(df_matches['uid']).symmetric_difference(set(df_footballdata['uid'])))\n",
    "    \n",
    "    # Merging both datasets\n",
    "    df_complete_league_year = pd.merge(df_matches, df_footballdata[['FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG',\n",
    "       'HTR', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY','AY', 'HR', 'AR','uid']], on='uid', how='inner')\n",
    "    \n",
    "    print(f'df_complete_league_year shape: {df_complete_league_year.shape}')\n",
    "\n",
    "    \n",
    "    \n",
    "    # Implementing models with Real Data as the guys from Stanford suggest.\n",
    "    # We create the dataset as it is stated at: \n",
    "    # *Shin, JongHo and Robert Gasparyan. “A novel way to Soccer Match Prediction.” (2014).*\n",
    "    \n",
    "    #Sort per date.\n",
    "    df_complete_league_year['date'] = pd.to_datetime(df_complete_league_year['date'])\n",
    "    df_complete_league_year=df_complete_league_year.sort_values(by='date')\n",
    "\n",
    "    #We keep just the data need it for this.\n",
    "    df_matches_info = df_complete_league_year[['id','uid','date','stage','home_team_name_id','away_team_name_id','FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG',\n",
    "       'HTR', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY','AY', 'HR', 'AR']]  \n",
    "    df_matches_info[['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF','HC', 'AC', 'HY','AY', 'HR', 'AR']] = df_matches_info.loc[:,['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF','HC', 'AC', 'HY','AY', 'HR', 'AR']].astype(float)\n",
    "\n",
    "    \n",
    "    #Creating a column for the labels\n",
    "    labels = df_matches_info[\"FTR\"]\n",
    "    df_matches_info = pd.get_dummies(df_matches_info, columns=[\"FTR\", \"HTR\"])\n",
    "    df_matches_info['labels'] = labels\n",
    "\n",
    "    \n",
    "    #Selecting features\n",
    "    df_matches_infos = df_matches_info[['id','stage','date','home_team_name_id','away_team_name_id','labels','FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF','HC', 'AC', 'HY','AY', 'HR', 'AR']]\n",
    "    df_matches_infos = df_matches_info[['uid','id','stage','date','home_team_name_id','away_team_name_id','labels','FTHG', 'FTAG']]\n",
    "\n",
    "   \n",
    "    return df_matches_infos\n",
    "\n",
    "\n",
    "datasets = []\n",
    "for league_id in leagues_id:\n",
    "    for season in seasons:\n",
    "        dataset_season = merge_matchtable_footballdata(season,league_id)\n",
    "        datasets.append(dataset_season.loc[:, :])\n",
    "        \n",
    "df_clustering = pd.concat(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Until here we have the origin dataset from which we are going to make the clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>id</th>\n",
       "      <th>stage</th>\n",
       "      <th>date</th>\n",
       "      <th>home_team_name_id</th>\n",
       "      <th>away_team_name_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bournemouthvilla</td>\n",
       "      <td>4390</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>bournemouth</td>\n",
       "      <td>villa</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chelseaswansea</td>\n",
       "      <td>4391</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>chelsea</td>\n",
       "      <td>swansea</td>\n",
       "      <td>D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evertonwatford</td>\n",
       "      <td>4392</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>everton</td>\n",
       "      <td>watford</td>\n",
       "      <td>D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leicestersunderland</td>\n",
       "      <td>4393</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>leicester</td>\n",
       "      <td>sunderland</td>\n",
       "      <td>H</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unitedtottenham</td>\n",
       "      <td>4394</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>united</td>\n",
       "      <td>tottenham</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   uid    id  stage       date home_team_name_id  \\\n",
       "1     bournemouthvilla  4390      1 2015-08-08       bournemouth   \n",
       "2       chelseaswansea  4391      1 2015-08-08           chelsea   \n",
       "3       evertonwatford  4392      1 2015-08-08           everton   \n",
       "4  leicestersunderland  4393      1 2015-08-08         leicester   \n",
       "5      unitedtottenham  4394      1 2015-08-08            united   \n",
       "\n",
       "  away_team_name_id labels  FTHG  FTAG  \n",
       "1             villa      A   0.0   1.0  \n",
       "2           swansea      D   2.0   2.0  \n",
       "3           watford      D   2.0   2.0  \n",
       "4        sunderland      H   4.0   2.0  \n",
       "5         tottenham      H   1.0   0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clustering.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the df where the value between teams is going to be our decision value to make clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>everton</th>\n",
       "      <th>liverpool</th>\n",
       "      <th>arsenal</th>\n",
       "      <th>southampton</th>\n",
       "      <th>sunderland</th>\n",
       "      <th>swansea</th>\n",
       "      <th>tottenham</th>\n",
       "      <th>norwich</th>\n",
       "      <th>chelsea</th>\n",
       "      <th>leicester</th>\n",
       "      <th>brom</th>\n",
       "      <th>crystal</th>\n",
       "      <th>city</th>\n",
       "      <th>stoke</th>\n",
       "      <th>villa</th>\n",
       "      <th>bournemouth</th>\n",
       "      <th>united</th>\n",
       "      <th>west</th>\n",
       "      <th>watford</th>\n",
       "      <th>newcastle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>everton</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liverpool</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arsenal</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southampton</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunderland</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            everton liverpool arsenal southampton sunderland swansea  \\\n",
       "everton         NaN       NaN     NaN         NaN        NaN     NaN   \n",
       "liverpool       NaN       NaN     NaN         NaN        NaN     NaN   \n",
       "arsenal         NaN       NaN     NaN         NaN        NaN     NaN   \n",
       "southampton     NaN       NaN     NaN         NaN        NaN     NaN   \n",
       "sunderland      NaN       NaN     NaN         NaN        NaN     NaN   \n",
       "\n",
       "            tottenham norwich chelsea leicester brom crystal city stoke villa  \\\n",
       "everton           NaN     NaN     NaN       NaN  NaN     NaN  NaN   NaN   NaN   \n",
       "liverpool         NaN     NaN     NaN       NaN  NaN     NaN  NaN   NaN   NaN   \n",
       "arsenal           NaN     NaN     NaN       NaN  NaN     NaN  NaN   NaN   NaN   \n",
       "southampton       NaN     NaN     NaN       NaN  NaN     NaN  NaN   NaN   NaN   \n",
       "sunderland        NaN     NaN     NaN       NaN  NaN     NaN  NaN   NaN   NaN   \n",
       "\n",
       "            bournemouth united west watford newcastle  \n",
       "everton             NaN    NaN  NaN     NaN       NaN  \n",
       "liverpool           NaN    NaN  NaN     NaN       NaN  \n",
       "arsenal             NaN    NaN  NaN     NaN       NaN  \n",
       "southampton         NaN    NaN  NaN     NaN       NaN  \n",
       "sunderland          NaN    NaN  NaN     NaN       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_teams = set(df_clustering['home_team_name_id'])\n",
    "\n",
    "df = pd.DataFrame(columns=name_teams, index= name_teams)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'H'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ab5a3ec50c78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#Clustering. We use K =5 because we want to distinct between(H+,H,D,A,A+)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_clustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clustering_teams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    969\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0;32m--> 311\u001b[0;31m                     order=order, copy=copy_x)\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'H'"
     ]
    }
   ],
   "source": [
    "first_clustering = 5\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print(i)\n",
    "    if i == 0:    \n",
    "        for team in name_teams:\n",
    "            df_clustering_teams =df_clustering.loc[df_clustering['home_team_name_id']== team,:]\n",
    "\n",
    "\n",
    "            ## Remove Rows With Missing Values\n",
    "            df_clustering_teams.dropna(inplace=True)\n",
    "\n",
    "            #Remove labels and also the columns that we don't want to use as clustering info\n",
    "            away_teams =df_clustering_teams['away_team_name_id']\n",
    "            df_clustering_teams = df_clustering_teams.drop(columns= ['uid','stage','labels','date','id','home_team_name_id','away_team_name_id'])\n",
    "\n",
    "\n",
    "            #Clustering. We use K =5 because we want to distinct between(H+,H,D,A,A+)\n",
    "            kmeans = KMeans(n_clusters=first_clustering, random_state=111)\n",
    "            kmeans.fit(df_clustering_teams)\n",
    "\n",
    "\n",
    "            #Plotting clustering in pca2 dimensions\n",
    "            pca = PCA(n_components=2).fit(df_clustering_teams)\n",
    "            pca_2d = pca.transform(df_clustering_teams)\n",
    "\n",
    "        #     fig, ax = plt.subplots(figsize=(15,15))\n",
    "        #     fig.suptitle(f'Home Team: {team}', fontsize=16)\n",
    "        #     ax.scatter(pca_2d[:, 0], pca_2d[:, 1], c=kmeans.labels_, s=1000)\n",
    "        #     for i, txt in enumerate(away_teams):\n",
    "        #         ax.annotate(txt, (pca_2d[i, 0], pca_2d[i, 1]))\n",
    "\n",
    "\n",
    "            df_clust = pd.DataFrame(columns=['team_away','cluster_labels'])\n",
    "            df_clust['team_away']= away_teams\n",
    "            df_clust['cluster_labels']= kmeans.labels_\n",
    "\n",
    "            #Filling our df with the values we want to use for clustering. In this particular case this values come from a previous clustering.\n",
    "            for away_team in away_teams:\n",
    "                df.loc[team,away_team] = df_clust.loc[df_clust['team_away']== away_team,'cluster_labels'].item()\n",
    "\n",
    "\n",
    "\n",
    "        #Fill NaN values due to teams playing to themselves, or due to first match day.\n",
    "        df =df.fillna('unknown')\n",
    "\n",
    "        #We should now transpose the matrix to have as a columns the \"performance calculated\" of the team in the row playing away against the team in the column playing at home.\n",
    "        dfT= df.T\n",
    "\n",
    "        dfT =dfT.astype('category')\n",
    "\n",
    "        print(dfT.info())\n",
    "\n",
    "        dfT = convert_to(dfT,'onehot', all_categorical = True,columns = 'None')\n",
    "\n",
    "        dfT.info()  \n",
    "\n",
    "        #Now we make the clustering for those rows. This clustering wants to cluster teams that have same caracteristics playing away.\n",
    "        name_teams = set(dfT.index)\n",
    "\n",
    "\n",
    "\n",
    "        Sum_of_squared_distances = []\n",
    "        K = range(1,15)\n",
    "        for k in K:\n",
    "            km = KMeans(n_clusters=k)\n",
    "            km = km.fit(dfT)\n",
    "            Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15,15))    \n",
    "        plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Sum_of_squared_distances')\n",
    "        plt.title('Elbow Method For Optimal k')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        kmeans = KMeans(n_clusters=6, random_state=111)\n",
    "        kmeans.fit(dfT)\n",
    "\n",
    "        pca = PCA(n_components=2).fit(dfT)\n",
    "        pca_2d = pca.transform(dfT)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "        ax.scatter(pca_2d[:, 0], pca_2d[:, 1], c=kmeans.labels_, s=1000)\n",
    "        \n",
    "\n",
    "        for i, txt in enumerate(name_teams):\n",
    "            ax.annotate(txt, (pca_2d[i, 0], pca_2d[i, 1]))\n",
    "    \n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        df_cluster = pd.DataFrame(columns=['team_away','cluster_team_away','team_home','cluster_team_home'])\n",
    "        df_cluster['team_away']= dfT.index\n",
    "        df_cluster['cluster_team_away']= kmeans.labels_ \n",
    "        print(df_cluster)\n",
    "\n",
    "\n",
    "    elif i ==1:\n",
    "        for team in name_teams:\n",
    "            df_clustering_teams =df_clustering.loc[df_clustering['away_team_name_id']== team,:]\n",
    "\n",
    "\n",
    "            ## Remove Rows With Missing Values\n",
    "            df_clustering_teams.dropna(inplace=True)\n",
    "\n",
    "            #Remove labels and also the columns that we don't want to use as clustering info\n",
    "            away_teams =df_clustering_teams['home_team_name_id']\n",
    "            df_clustering_teams = df_clustering_teams.drop(columns= ['uid','stage','labels','date','id','home_team_name_id','away_team_name_id'])\n",
    "\n",
    "\n",
    "            #Clustering. We use K =5 because we want to distinct between(H+,H,D,A,A+)\n",
    "            kmeans = KMeans(n_clusters=first_clustering, random_state=111)\n",
    "            kmeans.fit(df_clustering_teams)\n",
    "\n",
    "\n",
    "            #Plotting clustering in pca2 dimensions\n",
    "            pca = PCA(n_components=2).fit(df_clustering_teams)\n",
    "            pca_2d = pca.transform(df_clustering_teams)\n",
    "\n",
    "        #     fig, ax = plt.subplots(figsize=(15,15))\n",
    "        #     fig.suptitle(f'Home Team: {team}', fontsize=16)\n",
    "        #     ax.scatter(pca_2d[:, 0], pca_2d[:, 1], c=kmeans.labels_, s=1000)\n",
    "        #     for i, txt in enumerate(away_teams):\n",
    "        #         ax.annotate(txt, (pca_2d[i, 0], pca_2d[i, 1]))\n",
    "\n",
    "\n",
    "            df_clust = pd.DataFrame(columns=['team_away','cluster_labels'])\n",
    "            df_clust['team_away']= away_teams\n",
    "            df_clust['cluster_labels']= kmeans.labels_\n",
    "\n",
    "            #Filling our df with the values we want to use for clustering. In this particular case this values come from a previous clustering.\n",
    "            for away_team in away_teams:\n",
    "                df.loc[team,away_team] = df_clust.loc[df_clust['team_away']== away_team,'cluster_labels'].item()\n",
    "\n",
    "\n",
    "\n",
    "        #Fill NaN values due to teams playing to themselves, or due to first match day.\n",
    "        df =df.fillna('unknown')\n",
    "\n",
    "        #We should now transpose the matrix to have as a columns the \"performance calculated\" of the team in the row playing away against the team in the column playing at home.\n",
    "        dfT= df.T\n",
    "\n",
    "        dfT =dfT.astype('category')\n",
    "\n",
    "        print(dfT.info())\n",
    "\n",
    "        dfT = convert_to(dfT,'onehot', all_categorical = True,columns = 'None')\n",
    "\n",
    "        dfT.info()  \n",
    "\n",
    "        #Now we make the clustering for those rows. This clustering wants to cluster teams that have same caracteristics playing away.\n",
    "        name_teams = set(dfT.index)\n",
    "\n",
    "\n",
    "\n",
    "        Sum_of_squared_distances = []\n",
    "        K = range(1,15)\n",
    "        for k in K:\n",
    "            km = KMeans(n_clusters=k)\n",
    "            km = km.fit(dfT)\n",
    "            Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15,15))    \n",
    "        plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Sum_of_squared_distances')\n",
    "        plt.title('Elbow Method For Optimal k')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        kmeans = KMeans(n_clusters=3, random_state=111)\n",
    "        kmeans.fit(dfT)\n",
    "\n",
    "        pca = PCA(n_components=2).fit(dfT)\n",
    "        pca_2d = pca.transform(dfT)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "        ax.scatter(pca_2d[:, 0], pca_2d[:, 1], c=kmeans.labels_, s=1000)\n",
    "\n",
    "        for i, txt in enumerate(name_teams):\n",
    "            ax.annotate(txt, (pca_2d[i, 0], pca_2d[i, 1]))\n",
    "            \n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "        print(df_cluster)\n",
    "\n",
    "        df_cluster['team_home']= dfT.index\n",
    "        df_cluster['cluster_team_home']= kmeans.labels_   \n",
    "        print(df_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering['uid']= df_clustering['home_team_name_id']+df_clustering['away_team_name_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see if this cluster information alone can give us any useful information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering.head()\n",
    "df_testing_clustering = df_clustering[['uid','labels','home_team_name_id','away_team_name_id','date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_cluster_away(team_away,df_cluster):\n",
    "    clus_away = df_cluster.loc[df_cluster['team_away']==team_away, 'cluster_team_away'].item()\n",
    "    return clus_away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_cluster_home(team_home,df_cluster):\n",
    "    clus_home = df_cluster.loc[df_cluster['team_home']==team_home, 'cluster_team_home'].item()\n",
    "    return clus_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_clustering['cluster_team_away'] = df_testing_clustering.apply(lambda x: include_cluster_away(x['away_team_name_id'], df_cluster), axis =1 )\n",
    "\n",
    "\n",
    "df_testing_clustering['cluster_team_home'] = df_testing_clustering.apply(lambda x: include_cluster_home(x['home_team_name_id'], df_cluster), axis =1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_clustering.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_data = []\n",
    "for season in seasons:\n",
    "    seasons_data.append(season.replace('/','_').replace('20',''))\n",
    "\n",
    "location_to_file = 'data/'\n",
    "df_testing_clustering.to_csv(location_to_file+f'df_testing_clustering_app3_{leagues_id}_seasons_{seasons_data}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering.head()\n",
    "#df_clustering = df_clustering.drop(columns= ['home_team_name_id','away_team_name_id','id','stage'])\n",
    "df_clustering = df_clustering.drop(columns= ['home_team_name_id','away_team_name_id'])\n",
    "#df_clustering['dif_goals'] = df_clustering['FTHG']-df_clustering['FTAG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering.to_csv(location_to_file+f'df_FTHG_FTAG_{leagues_id}_seasons_{seasons_data}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_clustering.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfuzzy as fuzz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Set up the loop and plot\n",
    "fig1, axes1 = plt.subplots(3, 3, figsize=(10, 10))\n",
    "fpcs = []\n",
    "\n",
    "for ncenters, ax in enumerate(axes1.reshape(-1), 2):\n",
    "    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n",
    "        df_clustering, ncenters, 2, error=0.005, maxiter=1000, init=None)\n",
    "\n",
    "    # Store fpc values for later\n",
    "    fpcs.append(fpc)\n",
    "\n",
    "    # Plot assigned clusters, for each data point in training set\n",
    "    cluster_membership = np.argmax(u, axis=0)\n",
    "    for j in range(ncenters):\n",
    "        ax.plot(xpts[cluster_membership == j],\n",
    "                ypts[cluster_membership == j], '.', color=colors[j])\n",
    "\n",
    "    # Mark the center of each fuzzy cluster\n",
    "    for pt in cntr:\n",
    "        ax.plot(pt[0], pt[1], 'rs')\n",
    "\n",
    "    ax.set_title('Centers = {0}; FPC = {1:.2f}'.format(ncenters, fpc), size=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig1.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
